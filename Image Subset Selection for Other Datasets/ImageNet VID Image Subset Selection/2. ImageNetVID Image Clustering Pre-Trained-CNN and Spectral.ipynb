{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory with \"train\" and \"val\" folders\n",
    "base_dir = \"/vol/bitbucket/wr323/ILSVRC2017_VID_PROCESSED_SEP\"\n",
    "img_save_base_dir = \"/vol/bitbucket/wr323/ILSVRC2017_VID_Subsets_2frames/ResNet_SC\"\n",
    "\n",
    "result_save_dir = \"/vol/bitbucket/wr323/Result 2017 Pre-Trained-CNN and Spectral\"\n",
    "os.makedirs(result_save_dir, exist_ok=True)\n",
    "\n",
    "num_clusters = 4\n",
    "num_key_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "\n",
    "# Remove classification layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1]) \n",
    "model.to(device) \n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(video_dir):\n",
    "    images = []; filenames = []; image_paths = []\n",
    "    for filename in os.listdir(video_dir):\n",
    "        if filename.lower().endswith((\".jpeg\", \".jpg\", \".png\")):\n",
    "            img = Image.open(os.path.join(video_dir, filename))\n",
    "            images.append(img)\n",
    "            filenames.append(filename)\n",
    "            image_paths.append(os.path.join(video_dir, filename))\n",
    "    return images, filenames, image_paths\n",
    "\n",
    "def process_video_dir(video_dir, save_dir, result_save_dir, debugging = True):\n",
    "    images, filenames, image_paths = read_images(video_dir)\n",
    "\n",
    "    if len(images) - 1 < num_clusters:  # Handle case with fewer images than clusters\n",
    "        print(f\"Saving all frames from {video_dir} (found {len(images)}, need at least {num_clusters})\")\n",
    "        for image, filename in zip(images, filenames):\n",
    "            if debugging:\n",
    "                plt.imshow(closest_image)\n",
    "                plt.title(filename)\n",
    "                plt.show()\n",
    "            else:\n",
    "                source_path = os.path.join(video_dir, filename) \n",
    "                destination_path = os.path.join(save_dir, filename)\n",
    "                # print(source_path)\n",
    "                # print(destination_path)\n",
    "                # print('----------------')\n",
    "                shutil.copy2(source_path, destination_path)\n",
    "            # fig, axes = plt.subplots(1, num_key_clusters, figsize=(25, 5))\n",
    "            # axes[idx].imshow(closest_image)\n",
    "            # axes[idx].set_title(f\"Cluster {cluster_idx}: {filename}\")\n",
    "            # axes[idx].axis('off')\n",
    "        return  # Exit function early if fewer images than clusters\n",
    "\n",
    "    # Compute embedding for each image with 16 bins\n",
    "    embeddings = []\n",
    "    for img in images:\n",
    "        # print(type(img))\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            embedding = model(img_tensor)\n",
    "        embedding = embedding.cpu().squeeze().numpy()  \n",
    "        # print(len(embedding)) # 2048\n",
    "        embeddings.append(embedding.astype('double'))\n",
    "\n",
    "    # Flatten embeddings\n",
    "    flattened_embeddings = [embedding.reshape(1, -1) for embedding in embeddings]\n",
    "\n",
    "    # Concatenate flattened embeddings into one array\n",
    "    all_embeddings = np.concatenate(flattened_embeddings, axis=0)\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    spectral_cluster = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors', n_neighbors=num_clusters).fit(all_embeddings)\n",
    "\n",
    "    # Assign each embedding to a cluster\n",
    "    embedding_clusters = spectral_cluster.labels_\n",
    "    \n",
    "    # Count the number of images in each cluster\n",
    "    cluster_counts = Counter(embedding_clusters)\n",
    "\n",
    "    # Get the 5 largest clusters by their count\n",
    "    largest_clusters = cluster_counts.most_common(num_key_clusters)\n",
    "\n",
    "    print(largest_clusters)\n",
    "        \n",
    "    # Save results in the appropriate structure\n",
    "    non_empty_cluster_count = 0\n",
    "    closest_images = []\n",
    "\n",
    "    for idx, (cluster_idx, _) in enumerate(largest_clusters):\n",
    "        \n",
    "        cluster_embedding_img_filename = [(embedding, image, filename, image_path) for embedding, image, cluster, filename, image_path in zip(\n",
    "            embeddings, images, embedding_clusters, filenames, image_paths) if cluster == cluster_idx]\n",
    "        \n",
    "        if len(cluster_embedding_img_filename) > 0:\n",
    "            # print(len(cluster_embedding_img_filename))\n",
    "            non_empty_cluster_count += 1\n",
    "\n",
    "            cluster_embeddings = np.array([embedding for (embedding, _, _, _) in cluster_embedding_img_filename])\n",
    "            centroid = np.mean(cluster_embeddings, axis=0)\n",
    "\n",
    "            distances = [np.linalg.norm(embedding - centroid) for (embedding, _, _, _) in cluster_embedding_img_filename]\n",
    "            _, closest_image, filename, closest_image_path = cluster_embedding_img_filename[np.argmin(distances)]\n",
    "            # print(filename)\n",
    "            if debugging:\n",
    "                closest_images.append((closest_image, filename))\n",
    "            else: \n",
    "                destination_path = os.path.join(save_dir, filename)\n",
    "                # print(closest_image_path)\n",
    "                # print(destination_path)\n",
    "                # print('----------------')\n",
    "                shutil.copy2(closest_image_path, destination_path)\n",
    "\n",
    "    if debugging:\n",
    "        # Sort closest images by filename\n",
    "        closest_images.sort(key=lambda x: x[1])\n",
    "\n",
    "        # Display results in subplots\n",
    "        fig, axes = plt.subplots(1, num_key_clusters, figsize=(31, 5))  # 1 row, num_key_clusters columns\n",
    "\n",
    "        for idx, (image, filename) in enumerate(closest_images):\n",
    "            axes[idx].imshow(image)\n",
    "            axes[idx].set_title(f\"Cluster {embedding_clusters[idx]}: {filename}\")\n",
    "            axes[idx].axis('off')  # Turn off axis labels\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for better spacing\n",
    "        plt.show()\n",
    "        video_name = video_dir.split(\"/\")[-1]\n",
    "        fig.savefig(os.path.join(result_save_dir, f'{video_name}.jpg'))\n",
    "        # fig.close()\n",
    "\n",
    "    print(\"non_empty_cluster_count: \", non_empty_cluster_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging = False\n",
    "\n",
    "# Loop over train and val folders\n",
    "for split in [\"train\"]: # validation use full set\n",
    "    split_dir = os.path.join(base_dir, split)\n",
    "\n",
    "    # Loop over class folders within train/val\n",
    "    for class_name in os.listdir(split_dir):\n",
    "        class_dir = os.path.join(split_dir, class_name)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        # Loop over video folders within class folders\n",
    "        for video_name in os.listdir(class_dir):\n",
    "            if count == 3 and debugging:\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "            video_dir = os.path.join(class_dir, video_name)\n",
    "\n",
    "            # Create the save directory for this video within the new structure\n",
    "            save_dir = os.path.join(img_save_base_dir, split, class_name)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Process the video's frames\n",
    "            process_video_dir(video_dir, save_dir, result_save_dir, debugging=debugging)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
